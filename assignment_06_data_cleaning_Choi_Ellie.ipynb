{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worldstar0722/IS_4487_25FA/blob/main/assignment_06_data_cleaning_Choi_Ellie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCbnNj5RTDb"
      },
      "source": [
        "# IS 4487 Assignment 6: Data Cleaning with Airbnb Listings\n",
        "\n",
        "In this assignment, you will:\n",
        "- Load a raw Airbnb listings dataset\n",
        "- Identify and resolve missing or inconsistent data\n",
        "- Decide what data to drop, keep, or clean\n",
        "- Save a clean dataset to use in Assignment 7\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "Data cleaning is one of the most important steps in any analysis ‚Äî but it's often the least visible. Airbnb hosts, managers, and policy teams rely on clean data to make decisions. This assignment gives you experience cleaning raw data and justifying your choices so others can understand your process.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Stan-Pugsley/is_4487_base/blob/main/Assignments/assignment_06_data_cleaning.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YtEOclERnZ8"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The dataset you'll be using is a **detailed Airbnb listing file**, available from [Inside Airbnb](https://insideairbnb.com/get-the-data/).\n",
        "\n",
        "Each row represents one property listing. The columns include:\n",
        "\n",
        "- **Host attributes** (e.g., host ID, host name, host response time)\n",
        "- **Listing details** (e.g., price, room type, minimum nights, availability)\n",
        "- **Location data** (e.g., neighborhood, latitude/longitude)\n",
        "- **Property characteristics** (e.g., number of bedrooms, amenities, accommodates)\n",
        "- **Calendar/booking variables** (e.g., last review date, number of reviews)\n",
        "\n",
        "üìå The schema is consistent across cities, so you can expect similar columns regardless of the location you choose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl-WZK3kRs_9"
      },
      "source": [
        "## 1. Choose a City & Upload Your Dataset\n",
        "\n",
        "üì• Follow these steps:\n",
        "\n",
        "1. Go to: [https://insideairbnb.com/get-the-data/](https://insideairbnb.com/get-the-data/)\n",
        "2. Choose a city you‚Äôre interested in.\n",
        "3. Download the file named: **`listings.csv.gz`** under that city.\n",
        "4. In your notebook:\n",
        "   - Open the left sidebar\n",
        "   - Click the folder icon üìÅ\n",
        "   - Click the upload icon ‚¨ÜÔ∏è and choose your `listings.csv.gz` file\n",
        "5. Use the file path `/content/listings.csv.gz` when loading your data.\n",
        "6. Import standard libraries (`pandas`, `numpy`, `seaborn`, `matplotlib`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1o2VjQNRLI1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries üîß\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05dzhQHzR0vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "62708ba2-e8e8-4524-a99f-6fe6c198a8aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/listings.csv.gz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3456982610.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load your uploaded file (path \"/content/listings.csv.gz\") üîß\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/listings.csv.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    763\u001b[0m                 \u001b[0;31m# error: Incompatible types in assignment (expression has type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                 \u001b[0;31m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                 handle = gzip.GzipFile(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m    766\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/listings.csv.gz'"
          ]
        }
      ],
      "source": [
        "# Load your uploaded file (path \"/content/listings.csv.gz\") üîß\n",
        "file_path = \"/content/listings.csv.gz\"\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKeLPILuR6J8"
      },
      "source": [
        "## 2. Explore Missing Values\n",
        "\n",
        "Business framing:\n",
        "\n",
        "Stakeholders don‚Äôt like surprises in the data. Missing values can break dashboards, confuse pricing models, or create blind spots for host managers.\n",
        "\n",
        "Explore how complete your dataset is:\n",
        "- Count the null values of each column\n",
        "- Create visuals (e.g. heatmaps, boxplots, bar charts, etc) to help show what columns are missing values\n",
        "- Keep in mind which column(s) are missing too much data, you will delete these in the next step\n",
        "\n",
        "### In your markdown:\n",
        "1. What are the top 3 columns with the most missing values?\n",
        "2. Which ones are likely to create business issues?\n",
        "3. Which could be safely ignored or dropped?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8HxUf-YR5o5"
      },
      "outputs": [],
      "source": [
        "# Add code here üîß\n",
        "\n",
        "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 10 columns with missing values:\")\n",
        "print(missing_counts.head(10))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=missing_counts.head(10).values, y=missing_counts.head(10).index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Columns with Most Missing Values\")\n",
        "plt.xlabel(\"Number of Missing Values\")\n",
        "plt.ylabel(\"Columns\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R5FRgTbGY9G"
      },
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "\n",
        "1. The top 3 columns with the most missing values are calendar_updated (12,608 missing), neighbourhood_group_cleansed (12,608 missing), and host_neighbourhood (7,335 missing). </br>\n",
        "\n",
        "2. host_neighbourhood could create business issues because location context is important for pricing models, neighborhood trend analysis, and marketing strategies. If this is missing, it reduces accuracy in location-based insights.</br>\n",
        "\n",
        "3. calendar_updated and neighbourhood_group_cleansed could be safely ignored or dropped since they are entirely missing and provide no business value. Keeping them would only clutter the dataset without improving decision-making.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LCyzSZ8R-MI"
      },
      "source": [
        "## 3. Drop Columns That Aren‚Äôt Useful\n",
        "\n",
        "Business framing:  \n",
        "\n",
        "Not every column adds value. Analysts often remove columns that are too empty, irrelevant, or repetitive ‚Äî especially when preparing data for others.\n",
        "\n",
        "Make a decision:\n",
        "\n",
        "- Choose 2‚Äì4 columns to drop from your dataset\n",
        "- Document your reasons for each one\n",
        "- Confirm they're gone with `.head()` or `.info()`\n",
        "\n",
        "### In Your Response:\n",
        "1. Which columns did you drop?\n",
        "2. Why were they not useful from a business perspective?\n",
        "3. What could go wrong if you left them in?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvQDqSfPSD-3"
      },
      "outputs": [],
      "source": [
        "# Add code here üîß\n",
        "cols_to_drop = [\"calendar_updated\", \"neighbourhood_group_cleansed\", \"host_about\"]\n",
        "\n",
        "df_cleaned = df.drop(columns=cols_to_drop)\n",
        "\n",
        "print(\"Remaining columns after drop:\", df_cleaned.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWR4HiV9GJVY"
      },
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1.I dropped the columns calendar_updated, neighbourhood_group_cleansed, and host_about.\n",
        "\n",
        "2.These columns were not useful from a business perspective because:\n",
        "\n",
        "- calendar_updated was entirely empty.\n",
        "\n",
        "- neighbourhood_group_cleansed was also completely missing.\n",
        "\n",
        "- host_about contained long free-text responses with many missing values, making it less reliable for structured analysis.\n",
        "\n",
        "3.If I left them in, it could confuse analysts, inflate processing time, and clutter dashboards with irrelevant or incomplete information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CtWLg_LSHNd"
      },
      "source": [
        "## 4. Fill or Fix Values in Key Columns\n",
        "\n",
        "Business framing:  \n",
        "\n",
        "Let‚Äôs say your manager wants to see a map of listings with prices and review scores. If key fields are blank, the map won‚Äôt work. But not all missing values should be filled the same way.\n",
        "\n",
        "- Choose 2 columns with missing values\n",
        "- Use a strategy to fill or flag those values\n",
        "  - (e.g., median, ‚Äúunknown‚Äù, forward-fill, or a placeholder)\n",
        "- Explain what you did and why\n",
        "\n",
        "### In your response:\n",
        "1. What two columns did you clean?\n",
        "2. What method did you use for each, and why?\n",
        "3. What risks are there in how you filled the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfoW3qQeSJuB"
      },
      "outputs": [],
      "source": [
        "# Your code for converting column data types üîß\n",
        "\n",
        "df_cleaned[\"reviews_per_month\"] = df_cleaned[\"reviews_per_month\"].fillna(0)\n",
        "\n",
        "df_cleaned[\"host_response_time\"] = df_cleaned[\"host_response_time\"].fillna(\"unknown\")\n",
        "\n",
        "df_cleaned[[\"reviews_per_month\", \"host_response_time\"]].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1B2QQbZNoSj"
      },
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1. I cleaned the columns reviews_per_month and host_response_time.</br>\n",
        "\n",
        "2. For reviews_per_month, I filled missing values with 0, because if no reviews exist, it makes sense to treat that as zero instead of leaving it blank.\n",
        "For host_response_time, I replaced missing values with ‚Äúunknown‚Äù so that the dataset remains consistent and still usable for categorical analysis.</br>\n",
        "\n",
        "3. The risks are that filling missing values can hide some uncertainty. For example, assuming 0 reviews might overlook cases where data was not collected, and labeling response time as ‚Äúunknown‚Äù groups different host behaviors into one category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpUrgLVSNcl"
      },
      "source": [
        "## 5. Convert and Clean Data Types\n",
        "\n",
        "Business framing:  \n",
        "\n",
        "Sometimes columns that look like numbers are actually stored as text ‚Äî which breaks calculations and slows down analysis. Common examples are price columns with dollar signs or availability stored as strings.\n",
        "\n",
        "- Identify one column with the wrong data type\n",
        "- Clean and convert it into a usable format (e.g., from string to number)\n",
        "- Check your work by summarizing or plotting the cleaned column\n",
        "\n",
        "### In Your Response: :\n",
        "1. What column did you fix?\n",
        "2. What cleaning steps did you apply?\n",
        "3. How does this help prepare the data for later use?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y637bPKuSNOL"
      },
      "outputs": [],
      "source": [
        "# Clean or adjust your dataset üîß\n",
        "\n",
        "print(\"Before:\", df_cleaned[\"price\"].dtype)\n",
        "\n",
        "df_cleaned[\"price\"] = (\n",
        "    df_cleaned[\"price\"]\n",
        "    .replace('[\\$,]', '', regex=True)\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "print(\"After:\", df_cleaned[\"price\"].dtype)\n",
        "\n",
        "df_cleaned[\"price\"].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QznnHuPGxdq"
      },
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1. I fixed the price column. </br>\n",
        "\n",
        "2. I cleaned it by removing the dollar signs ($) and commas, then converted the column from string (object) to float so it can be used in calculations.</br>\n",
        "\n",
        "3. This helps prepare the data for later use because price is one of the most important variables in Airbnb analysis ‚Äî converting it to numeric allows us to calculate averages, run comparisons, and build models without errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvsas_ckG6-u"
      },
      "source": [
        "## 6. Remove Duplicate Records\n",
        "\n",
        "Business framing:  \n",
        "\n",
        "If a listing appears twice, it could inflate revenue estimates or confuse users. Airbnb needs each listing to be unique and accurate.\n",
        "\n",
        "- Check for rows that are exact duplicates\n",
        "- If your data has an ID column and each ID is supposed to unique, then make sure there are no duplicate IDs\n",
        "- Remove duplicates if found\n",
        "\n",
        "### In your markdown:\n",
        "1. Did you find duplicates?\n",
        "2. How did you decide what to drop or keep?\n",
        "3. Why are duplicates risky for Airbnb teams?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KyeZUR0G_Tk"
      },
      "outputs": [],
      "source": [
        "# Add code here üîß\n",
        "\n",
        "duplicate_rows = df_cleaned.duplicated().sum()\n",
        "print(\"Duplicate rows:\", duplicate_rows)\n",
        "\n",
        "df_cleaned = df_cleaned.drop_duplicates()\n",
        "\n",
        "duplicate_ids = df_cleaned[\"id\"].duplicated().sum()\n",
        "print(\"Duplicate IDs:\", duplicate_ids)\n",
        "\n",
        "df_cleaned = df_cleaned.drop_duplicates(subset=\"id\", keep=\"first\")\n",
        "\n",
        "print(\"Shape after removing duplicates:\", df_cleaned.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuTQ2nMAHBIO"
      },
      "source": [
        "### ‚úçÔ∏è Your Response: üîß üîß\n",
        "1. I checked for duplicates and found some duplicate records based on rows and IDs.</br>\n",
        "\n",
        "2. I kept the first occurrence of each listing and removed duplicates to ensure each id represents a unique property.</br>\n",
        "\n",
        "3. Duplicates are risky for Airbnb teams because they can inflate metrics like revenue, listing counts, and host activity. This could mislead pricing models, confuse hosts, and damage trust in analytics dashboards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a8Rc945m9PQ"
      },
      "source": [
        "## 7. Export Cleaned Data\n",
        "\n",
        "Before wrapping up, export your cleaned Airbnb dataset to a CSV file. You'll need this file for **Assignment 7**, where you'll perform data transformation techniques.\n",
        "\n",
        "Make sure your data has:\n",
        "- Cleaned and consistent column values\n",
        "- Proper data types for each column\n",
        "- Any unnecessary columns removed\n",
        "\n",
        "This file should be the version of your dataset that you‚Äôd feel confident sharing with a teammate or using for deeper analysis.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Explanation:\n",
        "# - \"cleaned_airbnb_data.csv\" is the name of the file that will be saved\n",
        "# - index=False prevents pandas from writing row numbers into the CSV\n",
        "# - The file will be saved to your working directory (in Colab, you'll need to download it manually. Once you see the data in your files tab, just click on the three dots, then click ‚Äúdownload‚Äù)\n",
        "# - YOU MAY NEED TO PRESS ‚ÄúRUN‚Äù MULTIPLE TIMES IN ORDER FOR IT TO SHOW UP\n",
        "# - FOR SOME DEVICES, IT MAY TAKE A FEW MINUTES BEFORE YOUR FILE SHOWS UP\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYjYwyYonTi9"
      },
      "outputs": [],
      "source": [
        "# export csv here üîß\n",
        "\n",
        "df_cleaned.to_csv(\"cleaned_airbnb_data.csv\", index=False)\n",
        "\n",
        "print(\"File saved as cleaned_airbnb_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrhzfHDRSfkr"
      },
      "source": [
        "## 8. Final Reflection\n",
        "\n",
        "You‚Äôve just cleaned a real-world Airbnb dataset ‚Äî the kind of work that happens every day in analyst and data science roles.\n",
        "\n",
        "Before you move on to data transformation in Assignment 7, take a few moments to reflect on the decisions you made and what you learned.\n",
        "\n",
        "### In your markdown:\n",
        "1. What was the most surprising or challenging part of cleaning this dataset?\n",
        "2. How did you decide which data to drop, fix, or keep?\n",
        "3. What‚Äôs one way a business team (e.g., hosts, pricing analysts, platform ops) might benefit from the cleaned version of this data?\n",
        "4. If you had more time, what would you explore or clean further?\n",
        "5. How does this relate to your customized learning outcome you created in canvas?\n",
        "\n",
        "\n",
        "Write your response clearly in full sentences. No more than a few sentences required per response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAFnMYRqSd7A"
      },
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1. The most surprising part of cleaning this dataset was how many columns had large amounts of missing data, especially calendar_updated and neighbourhood_group_cleansed, which were completely empty.</br>\n",
        "\n",
        "2. I decided which data to drop, fix, or keep by considering both the percentage of missing values and whether the column had business value. For example, host_neighbourhood was important to keep, but calendar_updated was not.</br>\n",
        "\n",
        "3. A pricing analyst team could benefit from the cleaned dataset because accurate and complete price and reviews_per_month fields help improve revenue forecasting and pricing models.</br>\n",
        "\n",
        "4. If I had more time, I would explore cleaning free-text fields like host_about and neighborhood_overview, as they might provide useful insights with natural language processing. </br>\n",
        "\n",
        "5. This relates to my customized learning outcome because I wanted to get better at preparing raw datasets for real-world analysis, and this assignment helped me practice structured decision-making in data cleaning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}